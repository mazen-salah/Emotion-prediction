{"cells":[{"cell_type":"markdown","metadata":{},"source":["<h2 align=center> Facial Expression Recognition</h2>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wvGxjjeV-9Ls","trusted":true},"outputs":[],"source":["import numpy as np\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import utils\n","import os\n","%matplotlib inline\n","\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras.layers import Dense, Input, Dropout,Flatten, Conv2D\n","from keras.layers import BatchNormalization, Activation, MaxPooling2D\n","from keras.models import Model, Sequential\n","from keras.optimizers import Adam\n","from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n","from keras.utils import plot_model\n","\n","import tensorflow as tf\n","from IPython.core.display import SVG, Image\n","from livelossplot import PlotLossesKerasTF\n","print(\"Tensorflow version:\", tf.__version__)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TalL_1Qr-9Qz","outputId":"f5fb9b05-976a-4979-ea23-33c3d87efb94","trusted":true},"outputs":[],"source":["for expression in os.listdir(\"input/facial-expression-dataset/train/train\"):\n","    print(str(len(os.listdir(\"input/facial-expression-dataset/train/train/\" + expression))) + \" \" + expression + \" images\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iri8ehFw-9Tj","outputId":"1cff3826-c0a9-41ff-a61b-5a677de101ae","trusted":true},"outputs":[],"source":["img_size = 48\n","batch_size = 64\n","\n","datagen_train = ImageDataGenerator(horizontal_flip=True)\n","\n","train_generator = datagen_train.flow_from_directory(\"input/facial-expression-dataset/train/train/\",\n","                                                    target_size=(img_size,img_size),\n","                                                    color_mode=\"grayscale\",\n","                                                    batch_size=batch_size,\n","                                                    class_mode='categorical',\n","                                                    shuffle=True)\n","\n","datagen_validation = ImageDataGenerator(horizontal_flip=True)\n","validation_generator = datagen_validation.flow_from_directory(\"input/facial-expression-dataset/test/test/\",\n","                                                    target_size=(img_size,img_size),\n","                                                    color_mode=\"grayscale\",\n","                                                    batch_size=batch_size,\n","                                                    class_mode='categorical',\n","                                                    shuffle=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"trusted":true},"outputs":[],"source":["# Initialising the CNN\n","model = Sequential()\n","\n","# 1 - Convolution\n","model.add(Conv2D(64,(3,3), padding='same', input_shape=(48, 48,1)))\n","model.add(BatchNormalization())\n","model.add(Activation('relu'))\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","model.add(Dropout(0.25))\n","\n","# 2nd Convolution layer\n","model.add(Conv2D(128,(5,5), padding='same'))\n","model.add(BatchNormalization())\n","model.add(Activation('relu'))\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","model.add(Dropout(0.25))\n","\n","# 3rd Convolution layer\n","model.add(Conv2D(512,(3,3), padding='same'))\n","model.add(BatchNormalization())\n","model.add(Activation('relu'))\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","model.add(Dropout(0.25))\n","\n","# 4th Convolution layer\n","model.add(Conv2D(512,(3,3), padding='same'))\n","model.add(BatchNormalization())\n","model.add(Activation('relu'))\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","model.add(Dropout(0.25))\n","\n","# Flattening\n","model.add(Flatten())\n","\n","# Fully connected layer 1st layer\n","model.add(Dense(256))\n","model.add(BatchNormalization())\n","model.add(Activation('relu'))\n","model.add(Dropout(0.25))\n","\n","# Fully connected layer 2nd layer\n","model.add(Dense(512))\n","model.add(BatchNormalization())\n","model.add(Activation('relu'))\n","model.add(Dropout(0.25))\n","\n","model.add(Dense(7, activation='softmax'))\n","\n","opt = Adam(lr=0.0005)\n","model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","%%time\n","\n","epochs = 15\n","steps_per_epoch = train_generator.n//train_generator.batch_size\n","validation_steps = validation_generator.n//validation_generator.batch_size\n","\n","reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1,\n","                              patience=2, min_lr=0.00001, mode='auto')\n","checkpoint = ModelCheckpoint(\"model_weights.h5\", monitor='val_accuracy',\n","                             save_weights_only=True, mode='max', verbose=1)\n","callbacks = [PlotLossesCallback(), checkpoint, reduce_lr]\n","\n","history = model.fit(\n","    x=train_generator,\n","    steps_per_epoch=steps_per_epoch,\n","    epochs=epochs,\n","    validation_data = validation_generator,\n","    validation_steps = validation_steps\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cHw8ir7CVAE0","trusted":true},"outputs":[],"source":["model_json = model.to_json()\n","model.save_weights('model_weights.h5')\n","with open(\"model.json\", \"w\") as json_file:\n","    json_file.write(model_json)"]},{"cell_type":"code","execution_count":6,"metadata":{"trusted":true},"outputs":[],"source":["import cv2\n","import numpy as np\n","\n","facec = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n","import tensorflow as tf\n","import numpy as np\n","\n","class FacialExpressionModel(object):\n","    EMOTIONS_LIST = [\"Angry\", \"Disgust\",\n","                     \"Fear\", \"Happy\",\n","                     \"Neutral\", \"Sad\",\n","                     \"Surprise\"]\n","\n","    def __init__(self, model_json_file, model_weights_file):\n","        with open(model_json_file, \"r\") as json_file:\n","            loaded_model_json = json_file.read()\n","            self.loaded_model = tf.keras.models.model_from_json(loaded_model_json)\n","\n","        if self.loaded_model is not None:\n","            self.loaded_model.load_weights(model_weights_file)\n","\n","    import cv2\n","    import numpy as np\n","\n","    facec = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n","    import tensorflow as tf\n","    import numpy as np\n","\n","    class FacialExpressionModel(object):\n","        EMOTIONS_LIST = [\"Angry\", \"Disgust\",\n","                         \"Fear\", \"Happy\",\n","                         \"Neutral\", \"Sad\",\n","                         \"Surprise\"]\n","\n","        def __init__(self, model_json_file, model_weights_file):\n","            with open(model_json_file, \"r\") as json_file:\n","                loaded_model_json = json_file.read()\n","                self.loaded_model = tf.keras.models.model_from_json(loaded_model_json)\n","\n","            if self.loaded_model is not None:\n","                self.loaded_model.load_weights(model_weights_file)\n","\n","        def predict_emotion(self, img):\n","            if self.loaded_model is not None:\n","                self.preds = self.loaded_model.predict(img)\n","                return FacialExpressionModel.EMOTIONS_LIST[np.argmax(self.preds)]\n","            else:\n","                return None\n","\n","font = cv2.FONT_HERSHEY_SIMPLEX\n","\n","class VideoCamera(object):\n","    def __init__(self):\n","        self.video = cv2.VideoCapture(0)\n","\n","    def __del__(self):\n","        self.video.release()\n","\n","    # returns camera frames along with bounding boxes and predictions\n","    def get_frame(self):\n","        _, fr = self.video.read()\n","        gray_fr = cv2.cvtColor(fr, cv2.COLOR_BGR2GRAY)\n","        faces = facec.detectMultiScale(gray_fr, 1.3, 5)\n","        \n","        \n","\n","        for (x, y, w, h) in faces:\n","            fc = gray_fr[y:y+h, x:x+w]\n","\n","            roi = cv2.resize(fc, (48, 48))\n","            pred = model.predict_emotion(roi[np.newaxis, :, :, np.newaxis])\n","\n","            if pred is not None:\n","                cv2.putText(fr, str(pred), (x, y), font, 1, (255, 255, 0), 2)\n","            cv2.rectangle(fr,(x,y),(x+w,y+h),(255,0,0),2)\n","\n","        return fr"]},{"cell_type":"code","execution_count":11,"metadata":{"trusted":true},"outputs":[],"source":["import cv2\n","\n","\n","def gen(camera):\n","    while True:\n","        success, frame = camera.read()\n","        cv2.imshow('frame', frame)\n","        if not success:\n","            break\n","        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n","        yield gray\n","\n","\n","# mediapipe"]},{"cell_type":"code","execution_count":12,"metadata":{"trusted":false},"outputs":[],"source":["camera = VideoCamera()\n","generator = gen(camera)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.0"}},"nbformat":4,"nbformat_minor":4}
